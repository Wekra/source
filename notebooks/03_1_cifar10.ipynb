{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Object Classification\n",
    "\n",
    "The CIFAR-10 dataset contains 60k 32x32 pixel color images from 10 different classes.\n",
    "\n",
    "The classes are:\n",
    "- airplane \n",
    "- automobile \n",
    "- bird \n",
    "- cat \n",
    "- deer \n",
    "- dog \n",
    "- frog \n",
    "- horse \n",
    "- ship \n",
    "- truck\n",
    "\n",
    "Tasks:\n",
    "\n",
    "- implement the TODOs\n",
    "- train a MLP to achieve >40% test accuracy\n",
    "- add TensorBoard summaries\n",
    "- train a CNN to achieve >80% test accuracy\n",
    "\n",
    "Help:\n",
    "- use the TensorFlow API Documentation [https://www.tensorflow.org/api_docs/](https://www.tensorflow.org/api_docs/)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# download CIFAR-10\n",
    "wget -q https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# unpack\n",
    "tar xzf cifar-10-python.tar.gz\n",
    "# remove tar.gz\n",
    "rm cifar-10-python.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to unpickle data files\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        obj = pickle.load(fo, encoding='bytes')\n",
    "    return obj\n",
    "\n",
    "# function to store  data in pickle file\n",
    "def store(obj, filename):\n",
    "    pickle.dump(obj, open('cifar-10-batches-py/' + filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-code:\n",
    "```python\n",
    "x_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(1)).get(bytes('data', 'ascii'))\n",
    "img = x_train_b[1]\n",
    "print(img.shape)\n",
    "r = img[0:1024]\n",
    "g = img[1024:2048]\n",
    "b = img[2048:3072]\n",
    "print(r.shape)\n",
    "print(g.shape)\n",
    "print(b.shape)\n",
    "rgb = np.dstack((r,g,b))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decode pickle data as images\n",
    "# see https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def decode_as_image(img_flat):\n",
    "    img_R = img_flat[0:1024]\n",
    "    img_G = img_flat[1024:2048]\n",
    "    img_B = img_flat[2048:3072]\n",
    "    return np.dstack((img_R, img_G, img_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data and save to disk for later usage\n",
    "# note: you might need to give Docker more memory\n",
    "# alternatively, execute separately\n",
    "x_train = []\n",
    "for i in range(1, 6):\n",
    "    x_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(i)).get(bytes('data', 'ascii'))\n",
    "    for img in x_train_b:\n",
    "        img = decode_as_image(img)\n",
    "        x_train.append(img)\n",
    "\n",
    "# reshape the data\n",
    "x_train = np.array(x_train).reshape(5*10000, 32*32, 3)\n",
    "\n",
    "# save to disk\n",
    "store(x_train, 'x_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data and save to disk for later usage\n",
    "x_test = []\n",
    "x_test_b = unpickle('cifar-10-batches-py/test_batch').get(bytes('data', 'ascii'))\n",
    "for img in x_test_b:\n",
    "    img = decode_as_image(img)\n",
    "    x_test.append(img)\n",
    "\n",
    "# reshape the data\n",
    "x_test = np.array(x_test).reshape(1*10000, 32*32, 3)\n",
    "\n",
    "# save to disk\n",
    "store(x_test, 'x_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train labels and save to disk\n",
    "y_train = []\n",
    "for i in range(1, 6):\n",
    "    y_train_b = unpickle('cifar-10-batches-py/data_batch_' + str(i)).get(bytes('labels', 'ascii'))\n",
    "    for img in y_train_b:\n",
    "        y_train.append(img)\n",
    "        \n",
    "# reshape the data\n",
    "y_train = np.array(y_train).flatten()\n",
    "\n",
    "# save to disk\n",
    "store(y_train, 'y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test labels and save to disk\n",
    "y_test = []\n",
    "y_test_b = unpickle('cifar-10-batches-py/test_batch').get(bytes('labels', 'ascii'))\n",
    "for img in y_test_b:\n",
    "    y_test.append(img)\n",
    "        \n",
    "# reshape the data\n",
    "y_test = np.array(y_test).flatten()\n",
    "\n",
    "# save to disk\n",
    "store(y_test, 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = unpickle(\"cifar-10-batches-py/x_train\")\n",
    "x_test = unpickle(\"cifar-10-batches-py/x_test\")\n",
    "y_train = unpickle(\"cifar-10-batches-py/y_train\")\n",
    "y_test = unpickle(\"cifar-10-batches-py/y_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping from label number to label\n",
    "label_mapping = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "def get_label(i):\n",
    "    return label_mapping[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots the first 3 entries in the train set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rand = np.random.randint(50000 - 1)\n",
    "i = 0\n",
    "for idx in range(rand, rand + 3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.title(\"Class: {}\".format(get_label(int(y_train[idx]))))\n",
    "    plt.imshow(x_train[idx].reshape(32,32,3))\n",
    "    i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 32*32*3)\n",
    "x_test = x_test.reshape(-1, 32*32*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# TODO: normalize data and cast to float32\n",
    "x_train = x_train.astype(np.float32) / 255.0\n",
    "x_test = x_test.astype(np.float32) / 255.0\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define network parameters\n",
    "n_input = 3072 # image shape\n",
    "n_channels = 3 # number of channels\n",
    "n_classes = 10 # number of CIFAR-10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 9 9 ... 9 1 1]\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding of labels\n",
    "def one_hot_encode(a, length):\n",
    "    temp = np.zeros((a.shape[0], length))\n",
    "    temp[np.arange(a.shape[0]), a] = 1\n",
    "    return temp\n",
    "\n",
    "print(y_train)\n",
    "y_train = one_hot_encode(y_train.astype(np.int), n_classes)\n",
    "y_test = one_hot_encode(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_input], \"x\")\n",
    "y = tf.placeholder(tf.float32, [None, n_classes], \"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define hyper parameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 1000000\n",
    "batch_size = 256\n",
    "display_step = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [this](https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c) tutorial for the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # TODO: define MLP\n",
    "    flatten = tf.layers.flatten(x)\n",
    "    hidden1 = tf.layers.dense(inputs=flatten, units=255, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=hidden1, rate=0.4)\n",
    "    hidden2 = tf.layers.dense(inputs=dropout, units=255, activation=tf.nn.relu)\n",
    "    pred = tf.layers.dense(hidden2, 10, activation=tf.nn.softmax)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def cnn(x):\n",
    "    # TODO: define CNN\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, shape=(None, 32, 32, 3), name='input_x')\n",
    "    y =  tf.placeholder(tf.float32, shape=(None, 10), name='output_y')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    conv1_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 3, 64], mean=0, stddev=0.08))\n",
    "    conv2_filter = tf.Variable(tf.truncated_normal(shape=[3, 3, 64, 128], mean=0, stddev=0.08))\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 128, 256], mean=0, stddev=0.08))\n",
    "    conv4_filter = tf.Variable(tf.truncated_normal(shape=[5, 5, 256, 512], mean=0, stddev=0.08))\n",
    "\n",
    "    # 1, 2\n",
    "    conv1 = tf.nn.conv2d(x, conv1_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    conv1_pool = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv1_bn = tf.layers.batch_normalization(conv1_pool)\n",
    "\n",
    "    # 3, 4\n",
    "    conv2 = tf.nn.conv2d(conv1_bn, conv2_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    conv2_pool = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')    \n",
    "    conv2_bn = tf.layers.batch_normalization(conv2_pool)\n",
    "  \n",
    "    # 5, 6\n",
    "    conv3 = tf.nn.conv2d(conv2_bn, conv3_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    conv3_pool = tf.nn.max_pool(conv3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')  \n",
    "    conv3_bn = tf.layers.batch_normalization(conv3_pool)\n",
    "    \n",
    "    # 7, 8\n",
    "    conv4 = tf.nn.conv2d(conv3_bn, conv4_filter, strides=[1,1,1,1], padding='SAME')\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    conv4_pool = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "    conv4_bn = tf.layers.batch_normalization(conv4_pool)\n",
    "    \n",
    "    # 9\n",
    "    flat = tf.contrib.layers.flatten(conv4_bn)  \n",
    "\n",
    "    # 10\n",
    "    full1 = tf.contrib.layers.fully_connected(inputs=flat, num_outputs=128, activation_fn=tf.nn.relu)\n",
    "    full1 = tf.nn.dropout(full1, keep_prob)\n",
    "    full1 = tf.layers.batch_normalization(full1)\n",
    "    \n",
    "    # 11\n",
    "    full2 = tf.contrib.layers.fully_connected(inputs=full1, num_outputs=256, activation_fn=tf.nn.relu)\n",
    "    full2 = tf.nn.dropout(full2, keep_prob)\n",
    "    full2 = tf.layers.batch_normalization(full2)\n",
    "    \n",
    "    # 12\n",
    "    full3 = tf.contrib.layers.fully_connected(inputs=full2, num_outputs=512, activation_fn=tf.nn.relu)\n",
    "    full3 = tf.nn.dropout(full3, keep_prob)\n",
    "    full3 = tf.layers.batch_normalization(full3)    \n",
    "    \n",
    "    # 13\n",
    "    full4 = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=1024, activation_fn=tf.nn.relu)\n",
    "    full4 = tf.nn.dropout(full4, keep_prob)\n",
    "    full4 = tf.layers.batch_normalization(full4)        \n",
    "    \n",
    "    # 14\n",
    "    pred = tf.contrib.layers.fully_connected(inputs=full3, num_outputs=10, activation_fn=None)\n",
    "    \n",
    "    #x = tf.reshape(x, [-1, 3, 32,32]).transpose(0,2,3,1)\n",
    "    #conv1 = tf.layers.conv2d(inputs=x, filters=32, kernel_size=[5,5], data_format=\"channels_last\", activation=tf.nn.relu)\n",
    "    #pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=(3,3), strides=1)\n",
    "    #flatten = tf.layers.flatten(inputs=pool1)\n",
    "    #pred = tf.layers.dense(flatten, 10, activation=tf.nn.softmax)\n",
    "    #pred = tf.layers.dense(x, 10, activation=tf.nn.softmax)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-2740b02eddb8>:14: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From <ipython-input-16-2740b02eddb8>:15: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.max_pooling2d instead.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (?, 32, 32, 10) and (?, 10) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-516e5d85b7b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# define cost function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy\u001b[0;34m(onehot_labels, logits, weights, label_smoothing, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    790\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0monehot_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabel_smoothing\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \"\"\"\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (?, 32, 32, 10) and (?, 10) are incompatible"
     ]
    }
   ],
   "source": [
    "# build network\n",
    "#pred = mlp(x)\n",
    "pred = cnn(x)\n",
    "\n",
    "# define cost function and optimizer\n",
    "cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(y, pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# TODO: define tensorboard summaries\n",
    "#ith tf.name_scope('performance'):\n",
    "#   tf_loss_ph = tf.placeholder(tf.float32,shape=None,name='loss_summary')\n",
    "#   tf.summary.scalar('loss', tf_loss_ph)\n",
    "#  tf_accuracy_ph = tf.placeholder(tf.float32,shape=None, name='accuracy_summary')\n",
    "#   tf_accuracy_summary = tf.summary.scalar('accuracy', tf_accuracy_ph)\n",
    "\n",
    "train_acc = tf.summary.scalar('accuracy', accuracy)\n",
    "train_cost = tf.summary.scalar('cost', cost)\n",
    "\n",
    "merged = tf.summary.merge_all() # merges train acc and cost summaries\n",
    "\n",
    "test_acc = tf.summary.scalar('accuracy_test', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 998400/1000000 [============================>.] - ETA: 0s - loss: 2.0423 - acc: 0.4219\n",
      "\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.4257\n"
     ]
    }
   ],
   "source": [
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "progbar = tf.keras.utils.Progbar(training_iters, stateful_metrics=[\"loss\", \"acc\"])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter('tf-summary/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter('tf-summary/test')\n",
    "    step = 1\n",
    "    \n",
    "    # training loop\n",
    "    while step * batch_size < training_iters:\n",
    "        indices = np.random.randint(x_train.shape[0], size=batch_size)\n",
    "        batch_x = x_train[indices]\n",
    "        batch_y = y_train[indices]\n",
    "        # run optimization op (backprop)\n",
    "        if step % display_step == 0:\n",
    "            #print(\"Step:\", step)\n",
    "            # calculate train batch loss and accuracy\n",
    "            loss, acc, summary = sess.run([cost, accuracy, merged], feed_dict={x: batch_x, y: batch_y})\n",
    "            #loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y})\n",
    "            train_writer.add_summary(summary, step*batch_size)\n",
    "            \n",
    "            progbar.update(step*batch_size, values=[(\"loss\", loss), (\"acc\", acc)])\n",
    "            \n",
    "            # TODO: calculate test accuracy of random test batch\n",
    "            indices = np.random.randint(x_test.shape[0], size=batch_size)\n",
    "            test_batch_x = x_test[indices]\n",
    "            test_batch_y = y_test[indices]\n",
    "            acc = sess.run(test_acc, feed_dict={x: test_batch_x, y: test_batch_y})\n",
    "            test_writer.add_summary(acc, step*batch_size)\n",
    "        else:\n",
    "            #print(\"Step:\", step)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        step += 1\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print (\"Optimization Finished!\")\n",
    "    \n",
    "    # calculate accuracy for MNIST test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: x_test,\n",
    "                                      y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
